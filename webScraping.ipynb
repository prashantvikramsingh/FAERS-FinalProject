{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "# import luigi\n",
    "import sys\n",
    "import boto3\n",
    "from boto3.s3.transfer import S3Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractZip(url, source_dir, data_dir):\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    z = ZipFile(BytesIO(r.content))\n",
    "    z.extractall(source_dir)\n",
    "    moveFile(source_dir, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moveFile(source_dir, data_dir):\n",
    "    \n",
    "    RootDir1 = os.getcwd() + '\\\\' + source_dir\n",
    "    TargetFolder = os.getcwd() + '\\\\' + data_dir\n",
    "    for root, dirs, files in os.walk((os.path.normpath(RootDir1)), topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith('.csv'):\n",
    "                SourceFolder = os.path.join(root, name)\n",
    "                shutil.move(SourceFolder, TargetFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileLinks():\n",
    "\n",
    "    source_dir = \"FAERSsrc\"\n",
    "    data_dir = \"FAERSdata\"\n",
    "    host_url = \"http://www.nber.org\"\n",
    "    target_page = [\"http://www.nber.org/data/fda-adverse-event-reporting-system-faers-data.html\"]\n",
    "    \n",
    "    if not os.path.isdir(source_dir):\n",
    "        os.makedirs(source_dir)\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    for page_url in target_page:\n",
    "        try:\n",
    "            page_bs = BeautifulSoup(urlopen(page_url), \"lxml\")\n",
    "        except:\n",
    "            page_bs = BeautifulSoup(urlopen(page_url))\n",
    "\n",
    "        for url in page_bs.find_all(\"a\"):\n",
    "            a_string = str(url.string)\n",
    "\n",
    "            if \"csv\" in a_string:\n",
    "                url = host_url + url[\"href\"]\n",
    "                if (\"2018\" in str(url) or \"2017\" in str(url) or \"2016\" in str(url) or \"2015\" in str(url) \\\n",
    "                    or \"2014\" in str(url)) and (\"demo\" in str(url) or \"drug\" in str(url) or \"reac\" in str(url) \\\n",
    "                                                or \"outc\" in str(url)):\n",
    "                        extractZip(url, source_dir, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinedFile():\n",
    "\n",
    "    directoryPath = os.getcwd() + \"/FAERSdata\"\n",
    "    demo = pd.DataFrame(columns=['primaryid', 'caseid', 'mfr_dt', 'init_fda_dt', 'rept_cod', 'mfr_num', 'mfr_sndr', 'age',\n",
    "                                 'sex', 'wt', 'wt_cod', 'occp_cod', 'occr_country'])\n",
    "    drug = pd.DataFrame(columns=['primaryid', 'caseid', 'role_cod', 'drugname', 'route', 'dose_amt', 'dose_unit',\n",
    "                                 'dose_form', 'dose_freq'])\n",
    "    reaction = pd.DataFrame(columns=['primaryid', 'caseid', 'pt'])\n",
    "    outcome = pd.DataFrame(columns=['primaryid', 'caseid', 'outc_cod'])\n",
    "    print(\"Reading files and creating dataframes for each file type!!\", \"\\n\")\n",
    "    for filename in os.listdir(directoryPath):\n",
    "        if \"demo\" in filename:\n",
    "            demo_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            demo_df.drop(['caseversion', 'i_f_code', 'lit_ref', 'event_dt', 'auth_num', 'fda_dt', 'age_cod', 'age_grp',\n",
    "                          'e_sub', 'rept_dt', 'to_mfr', 'reporter_country'], inplace=True, axis=1, errors='ignore')\n",
    "            demo_df = demo_df.loc[(demo_df['wt_cod'] == 'KG')]\n",
    "            demo_df = demo_df[pd.notnull(demo_df['age'])]\n",
    "            demo_df = demo_df[1:]\n",
    "            demo = demo.append(demo_df, ignore_index=True)\n",
    "        if \"drug\" in filename:\n",
    "            durg_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            durg_df.drop(['drug_seq', 'val_vbm', 'dose_vbm', 'cum_dose_chr', 'prod_ai', 'cum_dose_unit', 'dechal',\n",
    "                          'rechal', 'lot_num', 'exp_dt', 'nda_num'], inplace=True, axis=1, errors='ignore')\n",
    "            durg_df = durg_df[pd.notnull(durg_df['dose_amt'])]\n",
    "            durg_df = durg_df[pd.notnull(durg_df['dose_unit'])]\n",
    "            durg_df = durg_df.loc[(durg_df['role_cod'] == 'PS')]\n",
    "            durg_df = durg_df[1:]\n",
    "            drug = drug.append(durg_df, ignore_index=True)\n",
    "        if \"reac\" in filename:\n",
    "            reac_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            reac_df = reac_df.groupby('primaryid')\n",
    "            reac_df = reac_df.filter(lambda x: len(x) == 1)\n",
    "            reac_df = reac_df[1:]\n",
    "            reaction = reaction.append(reac_df, ignore_index=True)\n",
    "        if \"outc\" in filename:\n",
    "            out_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            out_df = out_df.groupby('primaryid')\n",
    "            out_df = out_df.filter(lambda x: len(x) == 1)\n",
    "            out_df = out_df[1:]\n",
    "            outcome = outcome.append(out_df, ignore_index=True)\n",
    "\n",
    "    print(\"Dataframes created. Starting wrangling and combining files!!\", \"\\n\")        \n",
    "    demo_durg_df = pd.merge(drug, demo, on=('primaryid', 'caseid'), how='left')\n",
    "    demo_durg_df['sex'] = demo_durg_df['sex'].fillna('NS')\n",
    "    demodurgreact_df = pd.merge(demo_durg_df, reaction, on=('primaryid', 'caseid'), how='inner')\n",
    "    demodrugreactout_df = pd.merge(demodurgreact_df, outcome, on=('primaryid', 'caseid'), how='inner')\n",
    "    demodrugreactout_df.drop(['drug_rec_act'], inplace=True, axis=1, errors='ignore')\n",
    "    demodrugreactout_df['occp_cod'] = demodrugreactout_df['occp_cod'].fillna('OT')\n",
    "    demodrugreactout_df['rept_cod'] = demodrugreactout_df['rept_cod'].fillna('EXP')\n",
    "    demodrugreactout_df['mfr_sndr'] = demodrugreactout_df['mfr_sndr'].fillna('Others')\n",
    "    demodrugreactout_df['route'] = demodrugreactout_df['route'].fillna('Unknown')\n",
    "    demodrugreactout_df['dose_form'] = demodrugreactout_df['dose_form'].fillna('Others')\n",
    "    demodrugreactout_df['dose_freq'] = demodrugreactout_df['dose_freq'].fillna('Others')\n",
    "\n",
    "    demodrugreactout_df.to_csv('MergedFile.csv', header=True, index=False);\n",
    "    print(\"Combined file created\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileUploadToS3(AWS_ACCESS_KEY, AWS_SECRET_KEY):\n",
    "    \n",
    "    conn = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    transfer = S3Transfer(conn)\n",
    "\n",
    "    response = conn.list_buckets()    \n",
    "    existent = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        existent.append(bucket['Name'])\n",
    "\n",
    "    bucket_name = 'team1finalproject-faers'\n",
    "    #homepath = os.path.expanduser('~')\n",
    "    target_dir = './'\n",
    "    filenames = []\n",
    "    file_list = os.listdir(target_dir)\n",
    "    for file in file_list:\n",
    "        if '.csv' in file:\n",
    "            filenames.append(file)\n",
    "\n",
    "    if bucket_name in existent:\n",
    "        print('Bucket already exists!!', '\\n')\n",
    "        print('Combined File upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = files\n",
    "#             transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename, \\\n",
    "#                                  extra_args={'ACL': 'public-read'})\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename)\n",
    "        print('File uploaded to s3!!!!!','\\n')\n",
    "            \n",
    "    else:\n",
    "        print('Bucket not present. Creating bucket!!', '\\n')\n",
    "#        conn.create_bucket(Bucket=bucket_name, ACL='public-read-write')\n",
    "        conn.create_bucket(Bucket=bucket_name)\n",
    "        print('File upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = files\n",
    "#             transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename, \\\n",
    "#                                  extra_args={'ACL': 'public-read'})\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename)\n",
    "        print('File uploaded to s3!!!!!','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files and creating dataframes for each file type!! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashant\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes created. Starting wrangling and combining files!! \n",
      "\n",
      "Combined file created \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #getFileLinks()\n",
    "    getCombinedFile()\n",
    "    #fileUploadToS3('AKIAIRFUODTPVPPQTITA', '3Cn78F7sR9mpPvR/8RVeG5MuLhIHHm+CSkrnU7wZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
