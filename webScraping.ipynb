{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import boto3\n",
    "from boto3.s3.transfer import S3Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFiles(download_url, source_dir, data_dir):\n",
    "    \n",
    "    response = requests.get(download_url)\n",
    "    content = ZipFile(BytesIO(response.content))\n",
    "    content.extractall(source_dir)\n",
    "    moveFile(source_dir, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveFile(source_dir, data_dir):\n",
    "    \n",
    "    SourceFolder = os.getcwd() + '\\\\' + source_dir\n",
    "    TargetFolder = os.getcwd() + '\\\\' + data_dir\n",
    "    for root, dirs, files in os.walk((os.path.normpath(SourceFolder)), topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith('.csv'):\n",
    "                ActualFile = os.path.join(root, name)\n",
    "                shutil.move(ActualFile, TargetFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileLinks():\n",
    "\n",
    "    source_dir = \"FAERSsrc\"\n",
    "    data_dir = \"FAERSdata\"\n",
    "    host_url = \"http://www.nber.org\"\n",
    "    target_page = [\"http://www.nber.org/data/fda-adverse-event-reporting-system-faers-data.html\"]\n",
    "    \n",
    "    if not os.path.isdir(source_dir):\n",
    "        print('Creating Source Directory!!', '\\n')\n",
    "        os.makedirs(source_dir)\n",
    "        print('Source Directory Created!!', '\\n')\n",
    "    print('Source Directory already present!!', '\\n')\n",
    "    \n",
    "    if not os.path.isdir(data_dir):\n",
    "        print('Creating Target Directory!!', '\\n')\n",
    "        os.makedirs(data_dir)\n",
    "        print('Target Directory Created!!', '\\n')\n",
    "    print('Target Directory already present!!', '\\n')\n",
    "\n",
    "    for page_url in target_page:\n",
    "        print('Opening target page!!', '\\n')\n",
    "        try:\n",
    "            soup = BeautifulSoup(urlopen(page_url), \"lxml\")\n",
    "        except:\n",
    "            soup = BeautifulSoup(urlopen(page_url))\n",
    "            \n",
    "        print('Downloading Zip files!!', '\\n')\n",
    "\n",
    "        for url in soup.find_all(\"a\"):\n",
    "            a_string = str(url.string)\n",
    "\n",
    "            if \"csv\" in a_string:\n",
    "                download_url = host_url + url[\"href\"]\n",
    "                if (\"2018\" in str(download_url) or \"2017\" in str(download_url) or \"2016\" in str(download_url) or \n",
    "                    \"2015\" in str(download_url) or \"2014\" in str(download_url)) and (\"demo\" in str(download_url) or \n",
    "                                                                                     \"drug\" in str(download_url) or \n",
    "                                                                                     \"reac\" in str(download_url) or \n",
    "                                                                                     \"outc\" in str(download_url)):\n",
    "                    downloadFiles(download_url, source_dir, data_dir)\n",
    "        print('Files Downloaded, Unzipped and moved to Target Directory!!', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinedFile():\n",
    "\n",
    "    directoryPath = os.getcwd() + \"/FAERSdata\"\n",
    "    \n",
    "    demo = pd.DataFrame(columns=['primaryid', 'caseid', 'mfr_dt', 'init_fda_dt', 'rept_cod', 'mfr_num', 'mfr_sndr', 'age',\n",
    "                                 'sex', 'wt', 'wt_cod', 'occp_cod', 'occr_country'])\n",
    "    \n",
    "    drug = pd.DataFrame(columns=['primaryid', 'caseid', 'role_cod', 'drugname', 'route', 'dose_amt', 'dose_unit',\n",
    "                                 'dose_form', 'dose_freq'])\n",
    "    \n",
    "    reaction = pd.DataFrame(columns=['primaryid', 'caseid', 'pt'])\n",
    "    \n",
    "    outcome = pd.DataFrame(columns=['primaryid', 'caseid', 'outc_cod'])\n",
    "    \n",
    "    print(\"Reading files and creating dataframes for each file type!!\", \"\\n\")\n",
    "    \n",
    "    for filename in os.listdir(directoryPath):\n",
    "        if \"demo\" in filename:\n",
    "            demo_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            demo_df.drop(['caseversion', 'i_f_code', 'lit_ref', 'event_dt', 'auth_num', 'fda_dt', 'age_cod', 'age_grp',\n",
    "                          'e_sub', 'rept_dt', 'to_mfr', 'reporter_country'], inplace=True, axis=1, errors='ignore')\n",
    "            demo_df = demo_df.loc[(demo_df['wt_cod'] == 'KG')]\n",
    "            demo_df = demo_df[pd.notnull(demo_df['age'])]\n",
    "            demo_df = demo_df[1:]\n",
    "            demo = demo.append(demo_df, ignore_index=True)\n",
    "            \n",
    "        if \"drug\" in filename:\n",
    "            drug_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            drug_df.drop(['drug_seq', 'val_vbm', 'dose_vbm', 'cum_dose_chr', 'prod_ai', 'cum_dose_unit', 'dechal',\n",
    "                          'rechal', 'lot_num', 'exp_dt', 'nda_num'], inplace=True, axis=1, errors='ignore')\n",
    "            drug_df = drug_df[pd.notnull(drug_df['dose_amt'])]\n",
    "            drug_df = drug_df[pd.notnull(drug_df['dose_unit'])]\n",
    "            drug_df = drug_df.loc[(drug_df['role_cod'] == 'PS')]\n",
    "            drug_df = drug_df[1:]\n",
    "            drug = drug.append(drug_df, ignore_index=True)\n",
    "            \n",
    "        if \"reac\" in filename:\n",
    "            reac_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            reac_df = reac_df.groupby('primaryid')\n",
    "            reac_df = reac_df.filter(lambda x: len(x) == 1)\n",
    "            reac_df = reac_df[1:]\n",
    "            reaction = reaction.append(reac_df, ignore_index=True)\n",
    "            \n",
    "        if \"outc\" in filename:\n",
    "            out_df = pd.read_csv(directoryPath + \"/\" + filename, low_memory=False, sep=\",\", error_bad_lines=False)\n",
    "            out_df = out_df.groupby('primaryid')\n",
    "            out_df = out_df.filter(lambda x: len(x) == 1)\n",
    "            out_df = out_df[1:]\n",
    "            outcome = outcome.append(out_df, ignore_index=True)\n",
    "\n",
    "    print(\"Dataframes created. Starting wrangling and combining files!!\", \"\\n\")     \n",
    "    \n",
    "    demo_drug_df = pd.merge(drug, demo, on=('primaryid', 'caseid'), how='left')\n",
    "    demo_drug_df['sex'] = demo_drug_df['sex'].fillna('NS')\n",
    "    demodrugreac_df = pd.merge(demo_drug_df, reaction, on=('primaryid', 'caseid'), how='inner')\n",
    "    demodrugreacout_df = pd.merge(demodrugreac_df, outcome, on=('primaryid', 'caseid'), how='inner')\n",
    "    \n",
    "    demodrugreacout_df.drop(['drug_rec_act'], inplace=True, axis=1, errors='ignore')\n",
    "    demodrugreacout_df['occp_cod'] = demodrugreacout_df['occp_cod'].fillna('OT')\n",
    "    demodrugreacout_df['rept_cod'] = demodrugreacout_df['rept_cod'].fillna('EXP')\n",
    "    demodrugreacout_df['mfr_sndr'] = demodrugreacout_df['mfr_sndr'].fillna('Others')\n",
    "    demodrugreacout_df['route'] = demodrugreacout_df['route'].fillna('Unknown')\n",
    "    demodrugreacout_df['dose_form'] = demodrugreacout_df['dose_form'].fillna('Others')\n",
    "    demodrugreacout_df['dose_freq'] = demodrugreacout_df['dose_freq'].fillna('Others')\n",
    "\n",
    "    demodrugreacout_df.to_csv('MergedFile.csv', header=True, index=False);\n",
    "    print(\"Combined file created!!\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileUploadToS3(AWS_ACCESS_KEY, AWS_SECRET_KEY):\n",
    "    \n",
    "    conn = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    transfer = S3Transfer(conn)\n",
    "\n",
    "    response = conn.list_buckets()    \n",
    "    existent = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        existent.append(bucket['Name'])\n",
    "\n",
    "    bucket_name = 'team1finalproject-faers'\n",
    "    target_dir = './'\n",
    "    filenames = []\n",
    "    file_list = os.listdir(target_dir)\n",
    "    for file in file_list:\n",
    "        if '.csv' in file:\n",
    "            filenames.append(file)\n",
    "\n",
    "    if bucket_name in existent:\n",
    "        print('Bucket already exists!!', '\\n')\n",
    "        print('Combined File upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = files\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename, \\\n",
    "                                 extra_args={'ACL': 'public-read'})\n",
    "        print('File uploaded to s3!!!!!','\\n')\n",
    "            \n",
    "    else:\n",
    "        print('Bucket not present. Creating bucket!!', '\\n')\n",
    "        conn.create_bucket(Bucket=bucket_name)\n",
    "        print('File upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = files\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename, \\\n",
    "                                 extra_args={'ACL': 'public-read'})\n",
    "        print('File uploaded to s3!!!!!','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists!! \n",
      "\n",
      "Combined File upload started to s3!!!!! \n",
      "\n",
      "File uploaded to s3!!!!! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    getFileLinks()\n",
    "    getCombinedFile()\n",
    "    fileUploadToS3('AKIAJVCQNSVSLV3MM6QQ', 'wLF7jZ4yOqDLx3NROSA90V7ocT6lhBGlCSOl9iq0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
